# Project Overview

## ğŸ“ File Structure

```
MMLOSO/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ __init__.py              # Package initialization
â”‚   â”œâ”€â”€ config.py                # Configuration management
â”‚   â”œâ”€â”€ data_utils.py            # Data loading and preprocessing
â”‚   â”œâ”€â”€ tokenizer_utils.py       # Tokenizer extension utilities
â”‚   â”œâ”€â”€ trainer.py               # LoRA training utilities
â”‚   â”œâ”€â”€ evaluation.py            # Evaluation metrics
â”‚   â””â”€â”€ utils.py                 # Utility functions
â”œâ”€â”€ dataset/                      # Dataset directory
â”‚   â”œâ”€â”€ bhili-train.csv          # Hindi-Bhili training data
â”‚   â”œâ”€â”€ gondi-train.csv          # Hindi-Gondi training data
â”‚   â”œâ”€â”€ mundari-train.csv        # Hindi-Mundari training data
â”‚   â”œâ”€â”€ santali-train.csv        # English-Santali training data
â”‚   â””â”€â”€ test.csv                 # Test data
â”œâ”€â”€ models/                       # Model outputs (created during training)
â”‚   â”œâ”€â”€ tokenizer_extended/      # Extended tokenizer
â”‚   â””â”€â”€ checkpoints/             # Training checkpoints
â”œâ”€â”€ main.py                      # Main training script
â”œâ”€â”€ test_setup.py                # Setup test script
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ README.md                    # Main documentation
â”œâ”€â”€ QUICKSTART.md                # Quick start guide
â”œâ”€â”€ TRAINING_GUIDE.md            # Detailed training guide
â”œâ”€â”€ ARCHITECTURE.md              # Architecture details
â”œâ”€â”€ PROJECT_OVERVIEW.md          # This file
â””â”€â”€ problems.txt                 # Competition problem description
```

## ğŸ“‹ File Descriptions

### Core Source Files

#### `src/config.py`
- Configuration management
- Command-line argument parsing
- Configuration classes for model, LoRA, training, tokenizer, and data
- Path switching between local and Kaggle environments

#### `src/data_utils.py`
- Data loading from CSV files
- Data preprocessing and cleaning
- Dataset creation for translation tasks
- Train/validation splitting
- Support for multiple language pairs

#### `src/tokenizer_utils.py`
- Tokenizer extension utilities
- Token extraction from training data
- Script-aware token selection
- Filtering existing tokens
- Embedding initialization for new tokens

#### `src/trainer.py`
- LoRA model creation
- Model freezing utilities
- Training setup and execution
- Data collation for NLLB model

#### `src/evaluation.py`
- BLEU score computation
- chrF score computation
- Model evaluation utilities
- Competition score calculation

#### `src/utils.py`
- Utility functions
- Language code mapping
- File I/O utilities
- Validation functions

### Main Scripts

#### `main.py`
- Main entry point for training
- Orchestrates tokenizer extension, data preparation, training, and evaluation
- Supports multiple modes: `train`, `eval`, `extend_tokenizer`
- Handles path switching between local and Kaggle

#### `test_setup.py`
- Tests package installation
- Verifies data loading
- Tests tokenizer loading
- Validates configuration

### Documentation

#### `README.md`
- Main documentation
- High-level overview
- Architecture diagrams
- Installation instructions
- Usage examples
- Best practices
- Troubleshooting guide

#### `QUICKSTART.md`
- Quick start guide
- Basic usage examples
- Command-line arguments
- Recommended settings
- Troubleshooting tips

#### `TRAINING_GUIDE.md`
- Detailed training guide
- Step-by-step process
- Advanced techniques
- Common issues and solutions
- Performance tips

#### `ARCHITECTURE.md`
- Architecture details
- Component descriptions
- Data flow diagrams
- Performance considerations
- Best practices

## ğŸš€ Quick Start

1. **Install dependencies**:
```bash
pip install -r requirements.txt
```

2. **Test setup**:
```bash
python test_setup.py
```

3. **Extend tokenizer**:
```bash
python main.py --mode extend_tokenizer --local --new-tokens 200
```

4. **Train model**:
```bash
python main.py --mode train --local --epochs 10 --batch-size 8
```

## ğŸ”‘ Key Features

### 1. Safe Tokenizer Extension
- Preserves existing vocabulary
- Script-aware token selection
- Frequency-based filtering
- Priority for rare scripts

### 2. LoRA Fine-tuning
- Minimal trainable parameters (~0.11%)
- Prevents catastrophic forgetting
- Efficient training
- Configurable rank and alpha

### 3. Multi-language Support
- Handles multiple language pairs
- Different scripts (Devanagari, Ol Chiki, Roman)
- Language code mapping
- Proper tokenization

### 4. Path Switching
- Local environment: `./dataset`
- Kaggle environment: `/kaggle/input/mm-lo-so-2025`
- Automatic switching with `--local` flag

### 5. Comprehensive Evaluation
- BLEU score
- chrF score
- Competition score calculation
- Multi-direction evaluation

## ğŸ“Š Workflow

```
1. Extend Tokenizer
   â”œâ”€â”€ Load base tokenizer
   â”œâ”€â”€ Extract token candidates
   â”œâ”€â”€ Filter existing tokens
   â”œâ”€â”€ Select top N tokens
   â””â”€â”€ Add tokens (preserve existing)

2. Prepare Data
   â”œâ”€â”€ Load CSV files
   â”œâ”€â”€ Clean data
   â”œâ”€â”€ Split train/val
   â””â”€â”€ Create datasets

3. Initialize Model
   â”œâ”€â”€ Load base model
   â”œâ”€â”€ Extend embeddings
   â”œâ”€â”€ Initialize new tokens
   â”œâ”€â”€ Apply LoRA
   â””â”€â”€ Freeze base model

4. Train Model
   â”œâ”€â”€ Create data collator
   â”œâ”€â”€ Set up training arguments
   â”œâ”€â”€ Create trainer
   â”œâ”€â”€ Train model
   â””â”€â”€ Save checkpoints

5. Evaluate
   â”œâ”€â”€ Load model
   â”œâ”€â”€ Generate translations
   â”œâ”€â”€ Compute metrics
   â””â”€â”€ Calculate final score
```

## ğŸ¯ Usage Examples

### Extend Tokenizer
```bash
python main.py --mode extend_tokenizer --local --new-tokens 200
```

### Train Model
```bash
python main.py --mode train --local --epochs 10 --batch-size 8 --lora-r 16
```

### Evaluate
```bash
python main.py --mode eval --local
```

### Kaggle Environment
```bash
python main.py --mode train --epochs 10 --batch-size 8
```

## ğŸ”§ Configuration

### LoRA Configuration
- `r`: Rank (8, 16, or 32)
- `alpha`: Scaling factor (typically 2Ã— r)
- `dropout`: Dropout rate (0.1)
- `target_modules`: Attention and FFN layers

### Training Configuration
- `learning_rate`: 5e-5
- `batch_size`: 8
- `epochs`: 10
- `gradient_accumulation_steps`: 4
- `warmup_steps`: 500

### Tokenizer Configuration
- `new_tokens_count`: 200
- `min_frequency`: 2
- `preserve_existing`: True

## ğŸ“ˆ Performance

### Memory Usage
- Base model: ~2.3 GB (FP32) or ~1.2 GB (FP16)
- LoRA adapters: ~50 MB
- New token embeddings: ~10 MB
- Training: ~4-6 GB (with batch size 8)

### Training Time
- Tokenizer extension: ~5-10 minutes
- Model initialization: ~1-2 minutes
- Training (10 epochs, ~80k samples): ~2-4 hours (GPU)

### Inference Time
- Single sentence: ~50-100 ms
- Batch (16 sentences): ~200-300 ms

## ğŸ› Troubleshooting

### Common Issues
1. **Out of Memory**: Reduce batch size, use FP16
2. **Poor Performance**: Increase LoRA rank, add more tokens
3. **Catastrophic Forgetting**: Verify base model is frozen
4. **Tokenizer Extension Fails**: Check data files, verify encoding
5. **Language Code Errors**: Check language code mapping

### Solutions
- See `TRAINING_GUIDE.md` for detailed solutions
- Check `README.md` for common mistakes
- Review error messages carefully
- Test with smaller dataset first

## ğŸ“š Additional Resources

- [README.md](README.md): Main documentation
- [QUICKSTART.md](QUICKSTART.md): Quick start guide
- [TRAINING_GUIDE.md](TRAINING_GUIDE.md): Detailed training guide
- [ARCHITECTURE.md](ARCHITECTURE.md): Architecture details
- [problems.txt](problems.txt): Competition problem description

## ğŸ¤ Contributing

1. Read documentation
2. Test changes
3. Follow code style
4. Update documentation
5. Submit pull request

## ğŸ“„ License

[Your License Here]

## ğŸ™ Acknowledgments

- Facebook AI Research for NLLB model
- HuggingFace for transformers and PEFT libraries
- MMLoSo organizers for the dataset and challenge

