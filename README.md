# MMLoSo NMT Fine-tuning with LoRA

Comprehensive solution for fine-tuning NLLB-200-distilled-600M model for low-resource Indian languages (Bhili, Mundari, Gondi, Santali) using LoRA (Low-Rank Adaptation) and tokenizer extension.

## ğŸ§  High-level Overview

This project implements a complete pipeline for:

1. **Tokenizer Extension**: Safely add new vocabulary tokens (50-300 tokens) for low-resource languages without destroying existing vocabulary
2. **LoRA Fine-tuning**: Fine-tune the model using Low-Rank Adaptation to avoid catastrophic forgetting
3. **Multi-language Support**: Handle multiple language pairs with different scripts (Devanagari, Ol Chiki, Roman)
4. **Evaluation**: Comprehensive evaluation with BLEU and chrF metrics

## ğŸ§© Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Base NLLB-200 Model                      â”‚
â”‚              (facebook/nllb-200-distilled-600M)             â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Embeddings  â”‚  â”‚  Encoder     â”‚  â”‚  Decoder     â”‚     â”‚
â”‚  â”‚  (Extended)  â”‚  â”‚  (Frozen)    â”‚  â”‚  (Frozen)    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                  â”‚                  â”‚              â”‚
â”‚         â”‚                  â”‚                  â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                            â”‚                                 â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                  â”‚   LoRA Adapters   â”‚                       â”‚
â”‚                  â”‚  (Trainable Only) â”‚                       â”‚
â”‚                  â”‚                   â”‚                       â”‚
â”‚                  â”‚  - q_proj         â”‚                       â”‚
â”‚                  â”‚  - k_proj         â”‚                       â”‚
â”‚                  â”‚  - v_proj         â”‚                       â”‚
â”‚                  â”‚  - o_proj         â”‚                       â”‚
â”‚                  â”‚  - gate_proj      â”‚                       â”‚
â”‚                  â”‚  - up_proj        â”‚                       â”‚
â”‚                  â”‚  - down_proj      â”‚                       â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Training Flow:
1. Extend Tokenizer â†’ Add new tokens (preserve existing vocab)
2. Initialize Embeddings â†’ Average initialization for new tokens
3. Freeze Base Model â†’ Keep original 200 languages intact
4. Train LoRA + New Embeddings â†’ Only train adapters and new token embeddings
5. Evaluate â†’ BLEU + chrF metrics
```

## ğŸ“ Project Structure

```
MMLOSO/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Configuration management
â”‚   â”œâ”€â”€ data_utils.py          # Data loading and preprocessing
â”‚   â”œâ”€â”€ tokenizer_utils.py     # Tokenizer extension utilities
â”‚   â”œâ”€â”€ trainer.py             # LoRA training utilities
â”‚   â””â”€â”€ evaluation.py          # Evaluation metrics
â”œâ”€â”€ dataset/                   # Dataset directory
â”‚   â”œâ”€â”€ bhili-train.csv
â”‚   â”œâ”€â”€ gondi-train.csv
â”‚   â”œâ”€â”€ mundari-train.csv
â”‚   â”œâ”€â”€ santali-train.csv
â”‚   â””â”€â”€ test.csv
â”œâ”€â”€ models/                    # Model checkpoints
â”‚   â”œâ”€â”€ checkpoints/           # Training checkpoints
â”‚   â””â”€â”€ tokenizer_extended/    # Extended tokenizer
â”œâ”€â”€ main.py                    # Main training script
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # This file
â””â”€â”€ problems.txt               # Problem description
```

## ğŸ›  Installation

1. **Clone the repository** (if applicable)

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Verify installation**:
```bash
python -c "import torch; import transformers; import peft; print('All packages installed successfully!')"
```

## ğŸš€ Quick Start

### 1. Extend Tokenizer

```bash
# Local environment
python main.py --mode extend_tokenizer --local

# Kaggle environment
python main.py --mode extend_tokenizer
```

This will:
- Load training data from all language pairs
- Extract new token candidates (focusing on Ol Chiki and Devanagari scripts)
- Filter out existing tokens
- Add new tokens to tokenizer (preserving existing vocabulary)
- Save extended tokenizer to `./models/tokenizer_extended/`

### 2. Train Model

```bash
# Local environment
python main.py --mode train --local --epochs 10 --batch-size 8 --lora-r 16

# Kaggle environment
python main.py --mode train --epochs 10 --batch-size 8 --lora-r 16
```

### 3. Evaluate

```bash
python main.py --mode eval --local
```

## ğŸ“Š Configuration

### LoRA Configuration

**Recommended settings for low-resource languages**:

```python
# Conservative (small dataset, avoid overfitting)
lora_r = 8
lora_alpha = 16
lora_dropout = 0.1

# Balanced (recommended starting point)
lora_r = 16
lora_alpha = 32
lora_dropout = 0.1

# Aggressive (larger dataset, more capacity)
lora_r = 32
lora_alpha = 64
lora_dropout = 0.05
```

### Training Configuration

```python
# For ~20k samples per language pair
num_epochs = 10
batch_size = 8
gradient_accumulation_steps = 4
learning_rate = 5e-5
warmup_steps = 500
```

## ğŸ”¬ Key Features

### 1. Safe Tokenizer Extension

- **Preserves existing vocabulary**: Uses `add_tokens()` method which doesn't modify existing tokens
- **Smart token selection**: Prioritizes rare scripts (Ol Chiki, Devanagari variants)
- **Frequency filtering**: Only adds tokens that appear multiple times
- **Script-aware extraction**: Handles different scripts separately

### 2. Embedding Initialization

Three strategies available:

- **Average** (recommended): Initialize new token embeddings with average of existing embeddings
- **Random**: Initialize with small random values
- **Zero**: Initialize with zeros (not recommended)

### 3. LoRA Configuration

**Target modules** (for NLLB):
- `q_proj`, `k_proj`, `v_proj`, `o_proj` (attention layers)
- `gate_proj`, `up_proj`, `down_proj` (feed-forward layers)

**Why these modules?**
- Attention layers capture language-specific patterns
- Feed-forward layers handle semantic transformations
- Only ~0.1% of parameters are trainable (prevents overfitting)

### 4. Avoiding Catastrophic Forgetting

- **Freeze base model**: All original parameters remain frozen
- **Train only LoRA**: Only adapter weights are updated
- **Train new embeddings**: New token embeddings are trainable
- **Low learning rate**: Conservative updates (5e-5)

## âš ï¸ Common Mistakes and Solutions

### 1. **Tokenizer Extension Destroys Vocabulary**

**Problem**: Retraining SentencePiece from scratch loses existing vocabulary

**Solution**: Use `tokenizer.add_tokens()` which preserves existing tokens

```python
# âœ… Correct
tokenizer.add_tokens(new_tokens, special_tokens=False)

# âŒ Wrong
# Don't retrain SentencePiece from scratch
```

### 2. **New Tokens Not Initialized**

**Problem**: New token embeddings are random, hurting performance

**Solution**: Initialize with average of existing embeddings

```python
# âœ… Correct
avg_embedding = existing_embeddings.mean(dim=0)
new_embedding[token_id] = avg_embedding.clone()
```

### 3. **Catastrophic Forgetting**

**Problem**: Fine-tuning destroys original language capabilities

**Solution**: Use LoRA instead of full fine-tuning

```python
# âœ… Correct - LoRA (only ~0.1% parameters trainable)
peft_config = LoraConfig(r=16, alpha=32, ...)

# âŒ Wrong - Full fine-tuning (all parameters trainable)
# model.train()  # Don't do this!
```

### 4. **Wrong LoRA Target Modules**

**Problem**: Targeting wrong layers reduces effectiveness

**Solution**: Target attention and feed-forward layers

```python
# âœ… Correct
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                  "gate_proj", "up_proj", "down_proj"]

# âŒ Wrong
target_modules = ["embed_tokens"]  # Too limited
```

### 5. **Insufficient Data Augmentation**

**Problem**: Low-resource languages have limited data

**Solution**: Use data augmentation techniques (back-translation, synonym replacement)

## ğŸ”¬ Hacks to Improve Translation Quality

### 1. **Data Augmentation**

```python
# Back-translation
# 1. Train a reverse model (HRL â†’ LRL)
# 2. Translate high-resource data to low-resource
# 3. Add to training set

# Synonym replacement
# Replace words with synonyms in source language
```

### 2. **Curriculum Learning**

```python
# Start with easier examples (shorter sentences)
# Gradually increase difficulty
# Helps model learn basic patterns first
```

### 3. **Multi-task Learning**

```python
# Train on multiple language pairs simultaneously
# Shared representations help low-resource languages
# Use language tags to distinguish pairs
```

### 4. **Ensemble Methods**

```python
# Train multiple models with different seeds
# Average predictions at inference
# Improves robustness
```

### 5. **Script Normalization**

```python
# Normalize different script variants
# Handle Romanized vs native script
# Use script conversion libraries
```

### 6. **Few-shot Learning**

```python
# Use in-context learning with examples
# Provide few examples in prompt
# Model learns from context
```

## ğŸ“ˆ Evaluation

### Metrics

1. **BLEU Score**: Measures n-gram overlap
2. **chrF Score**: Character-level F-score (better for morphologically rich languages)

### Evaluation Formula (MMLoSo Competition)

```
Final Score = 0.6 * (0.6 * BLEU_forward + 0.4 * BLEU_reverse)
            + 0.4 * (0.6 * chrF_forward + 0.4 * chrF_reverse)
```

### Language Pairs

- **Forward** (LRL â†’ HRL): Bhiliâ†’Hindi, Mundariâ†’Hindi, Gondiâ†’Hindi, Santaliâ†’English
- **Reverse** (HRL â†’ LRL): Hindiâ†’Bhili, Hindiâ†’Mundari, Hindiâ†’Gondi, Englishâ†’Santali

## ğŸ¯ When to Use Different Training Strategies

### 1. **Adapter-Only Training** (Recommended for most cases)

**When**: 
- Limited data (< 50k samples)
- Want to preserve original model capabilities
- Need fast training

**Configuration**:
```python
freeze_embeddings = True  # Freeze all embeddings
train_only_lora = True    # Only train LoRA adapters
```

### 2. **Embedding-Only Training**

**When**:
- Many new tokens (> 500)
- New tokens are critical
- Limited compute

**Configuration**:
```python
freeze_base_model = True  # Freeze all base layers
train_only_embeddings = True  # Only train new token embeddings
```

### 3. **LoRA on Attention Layers**

**When**:
- Medium dataset (50k-200k samples)
- Need balance between capacity and stability
- Want to capture language-specific patterns

**Configuration**:
```python
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
lora_r = 16
lora_alpha = 32
```

### 4. **Full Fine-tuning** (Not Recommended)

**When**:
- Very large dataset (> 500k samples)
- Can afford risk of catastrophic forgetting
- Have resources for full retraining

**Configuration**:
```python
# Don't use LoRA, train all parameters
# High risk of catastrophic forgetting!
```

## ğŸ“ Script Handling Tips

### 1. **Ol Chiki Script (Santali)**

```python
# Ol Chiki Unicode range: U+1C50 to U+1C7F
# Ensure tokenizer can handle these characters
# May need to add special tokens for script markers
```

### 2. **Devanagari Script (Hindi, Bhili, Mundari, Gondi)**

```python
# Devanagari Unicode range: U+0900 to U+097F
# Handle compound characters (conjuncts)
# Normalize different variants
```

### 3. **Roman Script (English)**

```python
# Standard ASCII/Latin characters
# Handle case sensitivity
# Normalize punctuation
```

## ğŸ› Troubleshooting

### Issue: Out of Memory

**Solution**:
- Reduce batch size
- Increase gradient accumulation steps
- Use gradient checkpointing
- Use FP16/BF16

### Issue: Poor Translation Quality

**Solution**:
- Increase LoRA rank (r=32)
- Add more new tokens
- Train for more epochs
- Check data quality

### Issue: Catastrophic Forgetting

**Solution**:
- Reduce learning rate
- Increase LoRA dropout
- Add regularization
- Use smaller LoRA rank

## ğŸ“š References

- [NLLB Paper](https://arxiv.org/abs/2207.04672)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [PEFT Library](https://github.com/huggingface/peft)
- [MMLoSo Workshop](https://mm-loso.github.io/)

## ğŸ“„ License

[Your License Here]

## ğŸ‘¥ Contributors

[Your Name/Team]

## ğŸ™ Acknowledgments

- Facebook AI Research for NLLB model
- HuggingFace for transformers and PEFT libraries
- MMLoSo organizers for the dataset and challenge

